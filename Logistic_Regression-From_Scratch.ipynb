{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCCGqFD8MprpnzSHssrBHG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fawzy-AI-Explorer/X-From-Scratch/blob/main/Logistic_Regression-From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression From Scratct\n",
        "## Outline:\n",
        "- [Goals](#goals)\n",
        "- [Tools](#tools)\n",
        "- [Prediction && Accuracy](#pred_acc)\n",
        "- [Compute Cost](#cost)\n",
        "- [Gradient Descent](#grd_desc)\n",
        "- [Final Implementation](#final_implementation)\n",
        "- [Test the model](#test)\n",
        "- [Jupyter Notebook License](#license)"
      ],
      "metadata": {
        "id": "VbEKtzrMtk09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"goals\">Goals<a>\n",
        "In this toturial, we will:\n",
        "\n",
        "- Implement the logistic regression model.\n",
        "- Implement the gradient descent algorithm to train the model.\n",
        "- Implement the accuracy score to evaluate the model.<br><br>\n",
        "\n",
        "*All from scratch*"
      ],
      "metadata": {
        "id": "SqLxA5pAusjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"tools\">Tools<a>\n",
        "In this project, we will make use of:\n",
        "- math, This module provides access to the mathematical functions defined by the C standard\n",
        "- pandas, a Python library used for working with data sets\n",
        "- NumPy, a popular library for scientific computing\n",
        "- seaborn, a Python data visualization library based on matplotlib\n",
        "- Matplotlib, a popular library for plotting data"
      ],
      "metadata": {
        "id": "C88Qmwf0vCd1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "hThsZhi1suD2"
      },
      "outputs": [],
      "source": [
        "import math, copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"pred_acc\">Prediction && Accuracy<a>\n",
        "### Logistic Regression\n",
        "A logistic regression model applies the sigmoid to the familiar linear regression model as shown below:\n",
        "\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\tag{1} $$\n",
        "\n",
        "  where\n",
        "\n",
        "  $g(z) = \\frac{1}{1+e^{-z}}\\tag{2}$\n",
        "\n",
        "<br><br>\n",
        "**Accuracy Score**: This is the proportion of correct predictions among all predictions made by the model.<br><br>\n",
        "$$Accuracy=\\frac{Number\\ of\\ Correct\\ Predictions}{Total\\ Number\\ of\\ Predictions}\\tag{3}$$<br>"
      ],
      "metadata": {
        "id": "vX9Rde2W4cZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sigmoid function implementation\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Args:\n",
        "        z (ndarray): A scalar, numpy array of any size.\n",
        "\n",
        "    Returns:\n",
        "        g (ndarray): sigmoid(z), with the same shape as z\n",
        "    \"\"\"\n",
        "    # apply the sigmoid function\n",
        "    g = 1 / (1 + np.exp(-z))\n",
        "    return g"
      ],
      "metadata": {
        "id": "h_Yd32c_4c-o"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction function implemntation\n",
        "def predict(X, w, b):\n",
        "    \"\"\"\n",
        "    single predict using linear regression\n",
        "    Args:\n",
        "      X (ndarray): Shape (m, n) example with multiple features\n",
        "      w (ndarray): Shape (n,) model parameters\n",
        "      b (scalar):             model parameter\n",
        "\n",
        "    Returns:\n",
        "      p (scalar):  prediction\n",
        "    \"\"\"\n",
        "    # apply sigmiod and threshold\n",
        "    p = sigmoid(np.dot(w, X) + b)\n",
        "    return p"
      ],
      "metadata": {
        "id": "-GcRBoLi4nEv"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's make another function that predicts the exact category"
      ],
      "metadata": {
        "id": "4cf_1JcR4rz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predicts the exact value for the class\n",
        "def predict_class(X, w, b):\n",
        "    \"\"\"\n",
        "    single predict using linear regression\n",
        "    Args:\n",
        "      X (ndarray): Shape (m, n) example with multiple features\n",
        "      w (ndarray): Shape (n,) model parameters\n",
        "      b (scalar):             model parameter\n",
        "\n",
        "    Returns:\n",
        "      p (scalar):  prediction\n",
        "    \"\"\"\n",
        "    # apply sigmiod and threshold\n",
        "    p = 1. if sigmoid(np.dot(X, w) + b) >= 0.5 else 0.\n",
        "    return p"
      ],
      "metadata": {
        "id": "iHCTAKZC4rIK"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy function implementation\n",
        "def get_accuracy(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Returns the accuracy of the model\n",
        "    Args:\n",
        "    X (ndarray): Shape(m, n) examples with multiple features\n",
        "    y (ndarray): Shape (m,) the actual target values\n",
        "    w (ndarray): Shape (n,) model parameters\n",
        "    b (scalar) : model parameter\n",
        "\n",
        "    Returns:\n",
        "      accuracy (scalar): the accuracy of the model\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    correct = 0\n",
        "    # count the correct predictions\n",
        "    for i in range(m):\n",
        "      prediction = predict_class(X[i], w, b)\n",
        "      if prediction == y[i]:\n",
        "        correct += 1\n",
        "    # proportion of correct predictions among all predictions\n",
        "    accuracy = round(correct / m, 2)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "748JH2gq4xlZ"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a name=\"cost\">Compute Cost<a>\n",
        "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
        "$$J(\\mathbf{w},b) = -\\frac{1}{m}\\sum\\limits_{i=0}^{m-1}[y^{(i)} log(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})) + (1 - {y}^{(i)})log(1 - f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))]\\tag{1}$$"
      ],
      "metadata": {
        "id": "vT62LBX_46by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cost function for logistic regression\n",
        "def compute_cost(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function for logistic regression.\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples and n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (m,)): model parameters\n",
        "      b (scalar)    : model parameter\n",
        "\n",
        "    Returns:\n",
        "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
        "               to fit the data points in x and y\n",
        "    \"\"\"\n",
        "    # examples and features\n",
        "    m, n = X.shape\n",
        "    total_cost = 0.\n",
        "\n",
        "    # compute the loss for each example\n",
        "    for i in range(m):\n",
        "      epsilon = 1e-9 # to avoid overflow\n",
        "      f_wb_i = predict(X[i], w, b) + epsilon\n",
        "      total_cost += (y[i] * np.log(f_wb_i) + (1 - y[i]) * np.log(1 - f_wb_i))\n",
        "\n",
        "    total_cost /= -m\n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "DkZc-Is84zHZ"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"grd_desc\">Gradient Descent<a>\n",
        "Gradient descent for multiple variables:\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
        "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j = 0..n-1}\\newline\n",
        "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "\n",
        "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2}  \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}\n",
        "\\end{align}\n",
        "$$\n",
        "* m is the number of training examples in the data set\n",
        "\n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
      ],
      "metadata": {
        "id": "P8QdBPy75E1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "let's start by computing the gradients"
      ],
      "metadata": {
        "id": "G4Ru5FRi5Hcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to compute the gradients\n",
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters\n",
        "      b (scalar)       : model parameter\n",
        "\n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    # number of examples\n",
        "    m, n = X.shape\n",
        "\n",
        "    # initial gradients\n",
        "    dj_dw = np.zeros((n, ))\n",
        "    dj_db = 0.\n",
        "\n",
        "    # compute the actual gradient for each parameter\n",
        "    for i in range(m):\n",
        "      f_wb_i = predict(X[i], w, b)\n",
        "      err = f_wb_i - y[i]\n",
        "      dj_dw += err * X[i]\n",
        "      dj_db += err\n",
        "\n",
        "    dj_dw /= m\n",
        "    dj_db /= m\n",
        "\n",
        "    return dj_dw, dj_db"
      ],
      "metadata": {
        "id": "gzpf4mqS49vK"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's apply the algorithm"
      ],
      "metadata": {
        "id": "_p9SwPcU5PZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply gradient descent algorithm\n",
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n))   : Data, m examples with n features\n",
        "      y (ndarray (m,))    : target values\n",
        "      w_in (ndarray (n,)) : initial model parameters\n",
        "      b_in (scalar)       : initial model parameter\n",
        "      cost_function       : function to compute cost\n",
        "      gradient_function   : function to compute the gradient\n",
        "      alpha (float)       : Learning rate\n",
        "      num_iters (int)     : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,)) : Updated values of parameters\n",
        "      b (scalar)       : Updated value of parameter\n",
        "    \"\"\"\n",
        "    # initialize some variables\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)\n",
        "    b = b_in\n",
        "\n",
        "    # apply the algorithm `num_iters` of iterations\n",
        "    for i in range(num_iters):\n",
        "      # compute gradient\n",
        "      dj_dw, dj_db = gradient_function(X, y , w, b)\n",
        "      # simultneous update\n",
        "      w = w - alpha * dj_dw\n",
        "      b = b - alpha * dj_db\n",
        "\n",
        "      # save the history\n",
        "      if i < 100000:\n",
        "        J_history.append(cost_function(X, y, w, b))\n",
        "\n",
        "      # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "      if i % math.ceil(num_iters / 10) == 0:\n",
        "        print(\"{:>8} {:>11.5e}\".format(i, cost_function(X, y, w, b)))\n",
        "\n",
        "    return w, b, J_history\n"
      ],
      "metadata": {
        "id": "kvKcURir5Mu6"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"final_implementation\">Final Implementation<a>\n",
        "Now we will grab all this together to make the complete model that can be used easily later.<br><br>\n",
        "*NOTE*: we will rename some methods just to keep up with the original model"
      ],
      "metadata": {
        "id": "LDKTgHES5dHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Define a linear regressino class.\"\"\"\n",
        "\n",
        "\n",
        "class LogisticRegression():\n",
        "    \"\"\"Representation of a logistic regression model.\n",
        "    the model predicts categories from small number of possible outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=1e-3, n_iters=1000):\n",
        "      \"\"\"Initialize the linear regression model\n",
        "\n",
        "      Args:\n",
        "        learning_rate (scalar) : The number indicates the step in gradient descent.\n",
        "        n_iters (scalar) : The maximum number of passes over the training data.\n",
        "      \"\"\"\n",
        "      self.lr = learning_rate\n",
        "      self.n_iters = n_iters\n",
        "      self.weights = None\n",
        "      self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "      \"\"\"Fit the model according to the given training data.\n",
        "\n",
        "      Args:\n",
        "        X (ndarray (m,n))   : Data, m examples with n features\n",
        "        y (ndarray (m,))    : target values\n",
        "\n",
        "      Returns:\n",
        "        self (object) : Fitted model estimator.\n",
        "      \"\"\"\n",
        "      # initialize some variables\n",
        "      J_history = []\n",
        "      self.weights = np.zeros((X.shape[1],))\n",
        "      self.bias = 0.0\n",
        "\n",
        "      # apply the algorithm `num_iters` of iterations\n",
        "      for i in range(self.n_iters):\n",
        "        # compute gradient\n",
        "        dj_dw, dj_db = self.compute_gradient(X, y)\n",
        "        # simultneous update\n",
        "        self.weights = self.weights - self.lr * dj_dw\n",
        "        self.bias = self.bias - self.lr * dj_db\n",
        "\n",
        "        # save the history\n",
        "        if i < 100000:\n",
        "          J_history.append(self.compute_cost(X, y))\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i % math.ceil(self.n_iters / 10) == 0:\n",
        "          print(\"{:>8} {:>11.5e}\".format(i, self.compute_cost(X, y)))\n",
        "\n",
        "      return self\n",
        "\n",
        "\n",
        "    def compute_gradient(self, X, y):\n",
        "      \"\"\"\n",
        "      Computes the gradient for logistic regression\n",
        "      Args:\n",
        "        X (ndarray (m,n)): Data, m examples with n features\n",
        "        y (ndarray (m,)) : target values\n",
        "\n",
        "      Returns:\n",
        "        dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "        dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "      \"\"\"\n",
        "      # number of examples\n",
        "      m, n = X.shape\n",
        "\n",
        "      # initial gradients\n",
        "      dj_dw = np.zeros((n, ))\n",
        "      dj_db = 0.\n",
        "\n",
        "      # compute the actual gradient for each parameter\n",
        "      for i in range(m):\n",
        "        f_wb_i = self.predict(X[i])\n",
        "        err = f_wb_i - y[i]\n",
        "        dj_dw += err * X[i]\n",
        "        dj_db += err\n",
        "\n",
        "      dj_dw /= m\n",
        "      dj_db /= m\n",
        "\n",
        "      return dj_dw, dj_db\n",
        "\n",
        "    def predict(self, X):\n",
        "      \"\"\"\n",
        "      single predict using linear regression\n",
        "      Args:\n",
        "        X (ndarray): Shape (m, n) example with multiple features\n",
        "\n",
        "      Returns:\n",
        "        p (scalar):  prediction\n",
        "      \"\"\"\n",
        "      # apply sigmiod and threshold\n",
        "      p = self.sigmoid(np.dot(self.weights, X) + self.bias)\n",
        "      return p\n",
        "\n",
        "    def predict_class(self, X):\n",
        "      \"\"\"\n",
        "      single predict using linear regression\n",
        "      Args:\n",
        "        X (ndarray): Shape (m, n) example with multiple features\n",
        "\n",
        "      Returns:\n",
        "        p (scalar):  prediction\n",
        "      \"\"\"\n",
        "      # apply sigmiod and threshold\n",
        "      p = 1. if self.predict(X) >= 0.5 else 0.\n",
        "      return p\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "      \"\"\"\n",
        "      Compute the sigmoid of z\n",
        "\n",
        "      Args:\n",
        "          z (ndarray): A scalar, numpy array of any size.\n",
        "\n",
        "      Returns:\n",
        "          g (ndarray): sigmoid(z), with the same shape as z\n",
        "      \"\"\"\n",
        "      # apply the sigmoid function\n",
        "      g = 1 / (1 + np.exp(-z))\n",
        "      return g\n",
        "\n",
        "    def compute_cost(self, X, y):\n",
        "      \"\"\"\n",
        "      Computes the cost function for logistic regression.\n",
        "\n",
        "      Args:\n",
        "        X (ndarray (m,n)): Data, m examples and n features\n",
        "        y (ndarray (m,)): target values\n",
        "\n",
        "      Returns:\n",
        "          total_cost (float): The cost of using w,b as the parameters for linear regression\n",
        "                to fit the data points in x and y\n",
        "      \"\"\"\n",
        "      # examples and features\n",
        "      m, n = X.shape\n",
        "      total_cost = 0.\n",
        "\n",
        "      # compute the loss for each example\n",
        "      for i in range(m):\n",
        "        epsilon = 1e-9 # to avoid overflow\n",
        "        f_wb_i = self.predict(X[i]) + epsilon\n",
        "        total_cost += (y[i] * np.log(f_wb_i) + (1 - y[i]) * np.log(1 - f_wb_i))\n",
        "\n",
        "      total_cost /= -m\n",
        "\n",
        "      return total_cost\n",
        "\n",
        "    def score(self, X, y):\n",
        "      \"\"\"\n",
        "      Returns the accuracy of the model\n",
        "      Args:\n",
        "      X (ndarray): Shape(m, n) examples with multiple features\n",
        "      y (ndarray): Shape (m,) the actual target values\n",
        "\n",
        "      Returns:\n",
        "        accuracy (scalar): the accuracy of the model\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      correct = 0\n",
        "      # count the correct predictions\n",
        "      for i in range(m):\n",
        "        prediction = self.predict_class(X[i])\n",
        "        if prediction == y[i]:\n",
        "          correct += 1\n",
        "      # proportion of correct predictions among all predictions\n",
        "      accuracy = round(correct / m, 2)\n",
        "      return accuracy\n"
      ],
      "metadata": {
        "id": "qSsBvCcH86JG"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"test\">Test the model<a>\n",
        "Now let's test the model on [Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data)"
      ],
      "metadata": {
        "id": "i9f0UxKwDIq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will not go through the data exploration and analysis but if you want to know more you can [check this](https://www.kaggle.com/code/adham3lam/logistic-regression-from-scratch)."
      ],
      "metadata": {
        "id": "OUc02rN3EME-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "url = \"https://raw.githubusercontent.com/Ad7amstein/Logistic_Regression-Breast_Cancer_Diagnostic/main/data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# drop id and Unnamed: 32 columns\n",
        "df.drop([\"id\", \"Unnamed: 32\"], axis=1, inplace=True)\n",
        "\n",
        "# convert target to numerical values\n",
        "df.diagnosis = [1 if value == \"M\" else 0 for value in df.diagnosis]\n",
        "\n",
        "# divide into target variables and predictors\n",
        "y = df[\"diagnosis\"] # our target variable\n",
        "X = df.drop([\"diagnosis\"], axis=1) # our predictors\n",
        "X = X.values\n",
        "\n",
        "# Normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a calar object\n",
        "scalar = StandardScaler()\n",
        "\n",
        "# Fit the scalar to the data and transform it\n",
        "X_norm = scalar.fit_transform(X)\n",
        "\n",
        "# Split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.3, random_state=42)\n",
        "X_train, X_test, y_train, y_test = np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)"
      ],
      "metadata": {
        "id": "K2IBq-yjDYDb"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's build the model and train it."
      ],
      "metadata": {
        "id": "Yp-unIYzEuId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression()\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "print(\"The score of our model is {}%\".format(lr.score(X_test, y_test) * 100))"
      ],
      "metadata": {
        "id": "A9O_aWuaEK68",
        "outputId": "d78e4b8b-6fc7-4b2a-ad89-86d4a68d006b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       0 6.91224e-01\n",
            "     100 5.46879e-01\n",
            "     200 4.61833e-01\n",
            "     300 4.06128e-01\n",
            "     400 3.66671e-01\n",
            "     500 3.37111e-01\n",
            "     600 3.14027e-01\n",
            "     700 2.95418e-01\n",
            "     800 2.80035e-01\n",
            "     900 2.67062e-01\n",
            "The score of our model is 95.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"license\">Jupyter Notebook License<a>\n",
        "### Author: Adham Allam\n",
        "### How to reach me ?\n",
        "- <a href=\"https://www.kaggle.com/adham3lam\">kaggle<a>\n",
        "- <a href=\"https://www.linkedin.com/in/adham-allam-284486254/\">Linkedin<a>\n",
        "- <a href=\"https://linktr.ee/Adham.3llam\">Linktree<a>\n",
        "\n",
        "### Terms:\n",
        "1. Free to use for learning purposes.\n",
        "2. Attribution to the author, Adham Allam, is required.\n",
        "3. Permission to edit and explore, but derivative works must reference the original notebook and author.\n",
        "4. No warranties provided.<br>\n",
        "\n",
        "By using this notebook, you agree to these terms."
      ],
      "metadata": {
        "id": "nrNdBQsRN_aY"
      }
    }
  ]
}