{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdk5Q9QmUWZ076ajaNDOKp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fawzy-AI-Explorer/X-From-Scratch/blob/main/Linear_Regression-From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression From Scratch\n",
        "Table of content:\n",
        "- [Goals](#goals)\n",
        "- [Tools](#tools)\n",
        "- [Prediction && Accuracy](#predection_accuracy)\n",
        "- [Compute Cost With Multiple Variables](#cost)\n",
        "- [Gradient Descent With Multiple Variables](#gradient_descent)\n",
        "- [Final Implementation](#final_implementation)\n",
        "- [LICENSE](#license)"
      ],
      "metadata": {
        "id": "0kvJneJdrMQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"goals\">Goals<a>\n",
        "In this toturial, we will:\n",
        "\n",
        "- Implement the linear regression model\n",
        "- Implement the gradient descent algorithm to train the model\n",
        "- Implement $R^2 Score$ to calculate the accuracy<br><br>\n",
        "\n",
        "*All from scratch*"
      ],
      "metadata": {
        "id": "saplMwKGrVBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"tools\">Tools<a>\n",
        "In this toturial, we will make use of:\n",
        "- math, This module provides access to the mathematical functions defined by the C standard\n",
        "- pandas, a Python library used for working with data sets\n",
        "- NumPy, a popular library for scientific computing\n",
        "- seaborn, a Python data visualization library based on matplotlib\n",
        "- Matplotlib, a popular library for plotting data"
      ],
      "metadata": {
        "id": "d2VFcTdTseFO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Suy_k-6rq8gl"
      },
      "outputs": [],
      "source": [
        "import math, copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"predection_accuracy\">Prediction && Accuracy<a>\n",
        "We will build linear regression model so the prediction will be:<br>\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{1} $$\n",
        "<br><br>\n",
        "The most widely used formula to calculate the accuracy is r2_score so we will use it:\n",
        "$$R^2=1-\\frac{RSS}{TSS}\\tag{2}$$<br>\n",
        "Where:\n",
        "$$RSS=\\sum\\limits_{i = 0}^{m-1} (y^{(i)} - f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))^2 \\tag{3}$$<br>\n",
        "$$TSS=\\sum\\limits_{i = 0}^{m - 1} (y^{(i)} - y̅)^2\\tag{4}$$<br>\n",
        "$$y̅=\\frac{1}{m}\\sum\\limits_{i=0}^{m-1} (y^{(i)})\\tag{5}$$"
      ],
      "metadata": {
        "id": "QHPwrFml3g3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w, b):\n",
        "    \"\"\"\n",
        "    single predict using linear regression\n",
        "    Args:\n",
        "      X (ndarray): Shape (m, n) example with multiple features\n",
        "      w (ndarray): Shape (n,) model parameters\n",
        "      b (scalar):             model parameter\n",
        "\n",
        "    Returns:\n",
        "      p (scalar):  prediction\n",
        "     \"\"\"\n",
        "\n",
        "    p = np.dot(X, w) + b\n",
        "    return p\n"
      ],
      "metadata": {
        "id": "YlcA3TD13j8w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Returns the accuracy of the model\n",
        "    Args:\n",
        "    X (ndarray): Shape(m, n) examples with multiple features\n",
        "    y (ndarray): Shape (m,) the actual target values\n",
        "    w (ndarray): Shape (n,) model parameters\n",
        "    b (scalar) : model parameter\n",
        "\n",
        "    Returns:\n",
        "      accuracy (scalar): the accuracy of the model\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    RSS = 0.\n",
        "    TSS = 0.\n",
        "    y_mu = np.mean(y)\n",
        "\n",
        "    for i in range(m):\n",
        "        RSS += (y[i] - predict(X[i], w, b)) ** 2\n",
        "        TSS += (y[i] - y_mu) ** 2\n",
        "\n",
        "    score = 1 - RSS / TSS\n",
        "    print(\"The accuracy of our model is {}%\".format(round(score, 2) * 100))\n"
      ],
      "metadata": {
        "id": "EsK8BK_13opv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"cost\">Compute Cost With Multiple Variables<a>\n",
        "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{1}$$\n"
      ],
      "metadata": {
        "id": "pUmDNGKu3u0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function for linear regression.\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples and n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (m,)): model parameters\n",
        "      b (scalar)    : model parameter\n",
        "\n",
        "    Returns\n",
        "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
        "               to fit the data points in x and y\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "      f_wb_i = predict(X[i], w, b);\n",
        "      cost += (f_wb_i - y[i]) ** 2\n",
        "\n",
        "    cost /= (2 * m)\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "DfqmmFbr3v74"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"gradient_descent\">Gradient Descent With Multiple Variables<a>\n",
        "Gradient descent for multiple variables:\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
        "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j = 0..n-1}\\newline\n",
        "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "\n",
        "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2}  \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}\n",
        "\\end{align}\n",
        "$$\n",
        "* m is the number of training examples in the data set\n",
        "\n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
      ],
      "metadata": {
        "id": "ehG85VZc34kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters\n",
        "      b (scalar)       : model parameter\n",
        "\n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "      f_wb_i = predict(X[i], w, b)\n",
        "      err = (f_wb_i - y[i])\n",
        "      dj_dw = dj_dw + (err * X[i])\n",
        "      dj_db += err\n",
        "\n",
        "    dj_dw /= m\n",
        "    dj_db /= m\n",
        "\n",
        "    return dj_dw, dj_db\n"
      ],
      "metadata": {
        "id": "2Dl0sNGG37pr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n))   : Data, m examples with n features\n",
        "      y (ndarray (m,))    : target values\n",
        "      w_in (ndarray (n,)) : initial model parameters\n",
        "      b_in (scalar)       : initial model parameter\n",
        "      cost_function       : function to compute cost\n",
        "      gradient_function   : function to compute the gradient\n",
        "      alpha (float)       : Learning rate\n",
        "      num_iters (int)     : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,)) : Updated values of parameters\n",
        "      b (scalar)       : Updated value of parameter\n",
        "      \"\"\"\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    W_history = []\n",
        "    b_history = []\n",
        "    w = copy.deepcopy(w_in)\n",
        "    b = b_in\n",
        "    n = 1\n",
        "    if (len(X.shape) > 1):\n",
        "      n = X.shape[1]\n",
        "\n",
        "    for i in range(num_iters):\n",
        "      dj_dw, dj_db = gradient_function(X, y, w, b)\n",
        "      w = w - (alpha * dj_dw)\n",
        "      b -= alpha * dj_db\n",
        "\n",
        "      if i < 100000:\n",
        "        J_history.append(cost_function(X, y, w, b))\n",
        "        W_history.append(copy.deepcopy(w))\n",
        "        b_history.append(copy.deepcopy(b))\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i % math.ceil(num_iters / 10) == 0:\n",
        "          print(\"{:>8}  cost={:>1.5e}\".format(i, cost_function(X, y)), end='')\n",
        "          for i in range(n):\n",
        "            print(\"  w[{}]={:>1.1e}\".format(i, w[i]), end='')\n",
        "          print(\"  b={:>1.1e}\".format(b), end='')\n",
        "          for i in range(n):\n",
        "            print(\"  dj_dw[{}]={:>1.1e}\".format(i, dj_dw[i]), end='')\n",
        "          print(\"  dj_db={:>1.1e}\".format(dj_db))\n",
        "\n",
        "    return w, b, J_history, W_history, b_history\n"
      ],
      "metadata": {
        "id": "y4w4X_xK4BAM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"final_implementation\">Final Implementation<a>\n",
        "Now we will grap all this together to make the complete model that can be used easily later.<br><br>\n",
        "*NOTE*: we will rename some methods just to keep up with the original model"
      ],
      "metadata": {
        "id": "QFZpocLuESQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Define a linear regressino class.\"\"\"\n",
        "\n",
        "\n",
        "class LinearRegression():\n",
        "    \"\"\"Representation of a linear regression model.\n",
        "    the model predicts a number form  infinitely many possible outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=1e-4, max_iter=1000):\n",
        "      \"\"\"Initialize the linear regression model\n",
        "\n",
        "        Args:\n",
        "          learning_rage (float) : The number indicates the step in gradient descent.\n",
        "          max_iter (int) : The maximum number of passes over the training data.\n",
        "      \"\"\"\n",
        "\n",
        "      self.learning_rate = learning_rate\n",
        "      self.max_iter = max_iter\n",
        "      self.w, self.b = None, 0.\n",
        "\n",
        "    def fit(self, X, y, print_history=True):\n",
        "      \"\"\"\n",
        "      Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
        "      num_iters gradient steps with learning rate alpha\n",
        "\n",
        "      Args:\n",
        "        X (ndarray (m,n))   : Data, m examples with n features\n",
        "        y (ndarray (m,))    : target values\n",
        "\n",
        "      Returns:\n",
        "        self (object) : Fitted model estimator.\n",
        "        \"\"\"\n",
        "\n",
        "      # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "      J_history = []\n",
        "      W_history = []\n",
        "      b_history = []\n",
        "      n = 1\n",
        "      if (len(X.shape) > 1):\n",
        "        n = X.shape[1]\n",
        "      self.w = np.zeros((n, ))\n",
        "      self.b = 0.\n",
        "\n",
        "      for i in range(self.max_iter):\n",
        "        dj_dw, dj_db = self.compute_gradient(X, y)\n",
        "        self.w = self.w - (self.learning_rate * dj_dw)\n",
        "        self.b -= self.learning_rate * dj_db\n",
        "\n",
        "        if i < 100000:\n",
        "          J_history.append(self.compute_cost(X, y))\n",
        "          W_history.append(copy.deepcopy(self.w))\n",
        "          b_history.append(copy.deepcopy(self.b))\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i % math.ceil(self.max_iter / 10) == 0 and print_history:\n",
        "          print(\"{:>8}  cost={:>1.5e}\".format(i, self.compute_cost(X, y)), end='')\n",
        "          for i in range(n):\n",
        "            print(\"  w[{}]={:>1.1e}\".format(i, self.w[i]), end='')\n",
        "          print(\"  b={:>1.1e}\".format(self.b), end='')\n",
        "          for i in range(n):\n",
        "            print(\"  dj_dw[{}]={:>1.1e}\".format(i, dj_dw[i]), end='')\n",
        "          print(\"  dj_db={:>1.1e}\".format(dj_db))\n",
        "\n",
        "      return self\n",
        "\n",
        "\n",
        "    def compute_gradient(self, X, y):\n",
        "      \"\"\"\n",
        "      Computes the gradient for linear regression\n",
        "      Args:\n",
        "        X (ndarray (m,n)): Data, m examples with n features\n",
        "        y (ndarray (m,)) : target values\n",
        "\n",
        "      Returns:\n",
        "        dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "        dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "      \"\"\"\n",
        "\n",
        "      m = X.shape[0]\n",
        "      n = 1\n",
        "      if (len(X.shape) > 1):\n",
        "        n = X.shape[1]\n",
        "      dj_dw = np.zeros((n,))\n",
        "      dj_db = 0.\n",
        "\n",
        "      for i in range(m):\n",
        "        f_wb_i = self.predict(X[i])\n",
        "        err = (f_wb_i - y[i])\n",
        "        dj_dw = dj_dw + (err * X[i])\n",
        "        dj_db += err\n",
        "\n",
        "      dj_dw /= m\n",
        "      dj_db /= m\n",
        "\n",
        "      return dj_dw, dj_db\n",
        "\n",
        "\n",
        "    def compute_cost(self, X, y):\n",
        "      \"\"\"\n",
        "      Computes the cost function for linear regression.\n",
        "\n",
        "      Args:\n",
        "        X (ndarray (m,n)): Data, m examples and n features\n",
        "        y (ndarray (m,)): target values\n",
        "\n",
        "      Returns\n",
        "          total_cost (float): The cost of using w,b as the parameters for linear regression\n",
        "                to fit the data points in x and y\n",
        "      \"\"\"\n",
        "\n",
        "      m = X.shape[0]\n",
        "      cost = 0.0\n",
        "      for i in range(m):\n",
        "        f_wb_i = self.predict(X[i])\n",
        "        cost += (f_wb_i - y[i]) ** 2\n",
        "\n",
        "      cost /= (2 * m)\n",
        "\n",
        "      return cost\n",
        "\n",
        "    def predict(self, X):\n",
        "      \"\"\"\n",
        "      single predict using linear regression\n",
        "      Args:\n",
        "        X (ndarray): Shape (m, n) example with multiple features\n",
        "\n",
        "      Returns:\n",
        "        p (scalar):  prediction\n",
        "      \"\"\"\n",
        "\n",
        "      p = np.dot(X, self.w) + self.b\n",
        "      return p\n",
        "\n",
        "    def score(self, X, y):\n",
        "      \"\"\"\n",
        "      Returns the accuracy of the model\n",
        "      Args:\n",
        "      X (ndarray): Shape(m, n) examples with multiple features\n",
        "      y (ndarray): Shape (m,) the actual target values\n",
        "\n",
        "      Returns:\n",
        "        accuracy (scalar): the accuracy of the model\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = 1\n",
        "      if (len(X.shape) > 1):\n",
        "        n = X.shape[1]\n",
        "      RSS = 0.\n",
        "      TSS = 0.\n",
        "      y_mu = np.mean(y)\n",
        "\n",
        "      for i in range(m):\n",
        "          RSS += (y[i] - self.predict(X[i])) ** 2\n",
        "          TSS += (y[i] - y_mu) ** 2\n",
        "\n",
        "      score = 1 - RSS / TSS\n",
        "      return round(score, 2)\n",
        "\n",
        "    def get_params(self):\n",
        "      \"\"\"\n",
        "      Get parameters for this estimator.\n",
        "\n",
        "      Returns:\n",
        "        params (dict) : Parameter names mapped to their values.\n",
        "      \"\"\"\n",
        "      params = {}\n",
        "      params[\"learning_rate\"] = self.learning_rate\n",
        "      params[\"max_iter\"] = self.max_iter\n",
        "      if self.w is not None:\n",
        "        for i in range(len(self.w)):\n",
        "            p_name = \"w\" + str(i)\n",
        "            params[p_name] = self.w[i]\n",
        "      if self.b is not None:\n",
        "        params[\"b\"] = self.b\n",
        "\n",
        "      return params\n",
        "\n",
        "    def set_params(self, **params):\n",
        "      \"\"\"\n",
        "      Set the parameters of this estimator.\n",
        "\n",
        "      Args:\n",
        "        Estimator parameters.\n",
        "\n",
        "      Returns:\n",
        "        self (estimator instance) : Estimator instance.\n",
        "      \"\"\"\n"
      ],
      "metadata": {
        "id": "57Ab-TcG4Xdo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"test\">Test the model<a>\n",
        "Now let's test the model on the <a href=\"https://www.kaggle.com/datasets/kolawale/focusing-on-mobile-app-or-website\">ecommerce data set<a>"
      ],
      "metadata": {
        "id": "UtIFpJl6LiX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data set\n",
        "url = 'https://raw.githubusercontent.com/Ad7amstein/Linear_Regression-E-commerce/main/Ecommerce%20Customers.csv'\n",
        "data = pd.read_csv(url)\n",
        "# extract the four main features\n",
        "X = data.values[:, 3:7]\n",
        "y = data.values[:, 7]\n",
        "# split the data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# make instance form the model\n",
        "lm = LinearRegression(max_iter=100000, learning_rate=1e-4)\n",
        "# run the model on the training data\n",
        "lm.fit(X_train, y_train, print_history=True)\n",
        "# print the model parameters\n",
        "lm.get_params()\n",
        "# print the score\n",
        "print(\"The score of our model is {}%\".format(lm.score(X_test, y_test) * 100))"
      ],
      "metadata": {
        "id": "imhqR7VXo4-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a277ff-d321-43c3-e446-76a793054a9d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       0  cost=7.18746e+04  w[0]=1.7e+00  w[1]=6.1e-01  w[2]=1.9e+00  w[3]=1.9e-01  b=5.0e-02  dj_dw[0]=-1.7e+04  dj_dw[1]=-6.1e+03  dj_dw[2]=-1.9e+04  dj_dw[3]=-1.9e+03  dj_db=-5.0e+02\n",
            "   10000  cost=5.83830e+02  w[0]=1.0e+01  w[1]=2.3e+01  w[2]=-6.9e+00  w[3]=4.0e+01  b=-2.5e-01  dj_dw[0]=-2.0e+00  dj_dw[1]=-1.2e+01  dj_dw[2]=7.7e+00  dj_dw[3]=-2.2e+01  dj_db=4.3e-01\n",
            "   20000  cost=3.04426e+02  w[0]=1.1e+01  w[1]=3.1e+01  w[2]=-1.2e+01  w[3]=5.4e+01  b=-6.7e-01  dj_dw[0]=-6.2e-01  dj_dw[1]=-4.2e+00  dj_dw[2]=2.6e+00  dj_dw[3]=-7.3e+00  dj_db=4.2e-01\n",
            "   30000  cost=2.72291e+02  w[0]=1.2e+01  w[1]=3.3e+01  w[2]=-1.3e+01  w[3]=5.8e+01  b=-1.1e+00  dj_dw[0]=-1.9e-01  dj_dw[1]=-1.5e+00  dj_dw[2]=8.8e-01  dj_dw[3]=-2.4e+00  dj_db=4.2e-01\n",
            "   40000  cost=2.68449e+02  w[0]=1.2e+01  w[1]=3.4e+01  w[2]=-1.4e+01  w[3]=6.0e+01  b=-1.5e+00  dj_dw[0]=-6.3e-02  dj_dw[1]=-5.3e-01  dj_dw[2]=2.9e-01  dj_dw[3]=-8.1e-01  dj_db=4.2e-01\n",
            "   50000  cost=2.67851e+02  w[0]=1.2e+01  w[1]=3.4e+01  w[2]=-1.4e+01  w[3]=6.0e+01  b=-1.9e+00  dj_dw[0]=-2.3e-02  dj_dw[1]=-1.9e-01  dj_dw[2]=9.7e-02  dj_dw[3]=-2.7e-01  dj_db=4.2e-01\n",
            "   60000  cost=2.67627e+02  w[0]=1.2e+01  w[1]=3.5e+01  w[2]=-1.4e+01  w[3]=6.0e+01  b=-2.3e+00  dj_dw[0]=-1.1e-02  dj_dw[1]=-6.8e-02  dj_dw[2]=2.9e-02  dj_dw[3]=-9.0e-02  dj_db=4.2e-01\n",
            "   70000  cost=2.67447e+02  w[0]=1.2e+01  w[1]=3.5e+01  w[2]=-1.4e+01  w[3]=6.0e+01  b=-2.8e+00  dj_dw[0]=-7.2e-03  dj_dw[1]=-2.5e-02  dj_dw[2]=6.3e-03  dj_dw[3]=-3.0e-02  dj_db=4.2e-01\n",
            "   80000  cost=2.67271e+02  w[0]=1.2e+01  w[1]=3.5e+01  w[2]=-1.4e+01  w[3]=6.0e+01  b=-3.2e+00  dj_dw[0]=-6.1e-03  dj_dw[1]=-1.0e-02  dj_dw[2]=-1.6e-03  dj_dw[3]=-1.0e-02  dj_db=4.2e-01\n",
            "   90000  cost=2.67096e+02  w[0]=1.2e+01  w[1]=3.5e+01  w[2]=-1.4e+01  w[3]=6.0e+01  b=-3.6e+00  dj_dw[0]=-5.7e-03  dj_dw[1]=-4.7e-03  dj_dw[2]=-4.3e-03  dj_dw[3]=-3.7e-03  dj_db=4.2e-01\n",
            "The score of our model is 92.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"license\">Jupyter Notebook License<a>\n",
        "### Author: Adham Allam\n",
        "### How to reach me ?\n",
        "- <a href=\"https://www.linkedin.com/in/adham-allam-284486254/\">Linkedin<a>\n",
        "- <a href=\"https://linktr.ee/Adham.3llam\">Linktree<a>\n",
        "\n",
        "### Terms:\n",
        "1. Free to use for learning purposes.\n",
        "2. Attribution to the author, Adham Allam, is required.\n",
        "3. Permission to edit and explore, but derivative works must reference the original notebook and author.\n",
        "4. No warranties provided.<br>\n",
        "\n",
        "By using this notebook, you agree to these terms."
      ],
      "metadata": {
        "id": "RDxzd4bvHtIX"
      }
    }
  ]
}