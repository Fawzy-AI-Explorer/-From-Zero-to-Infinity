{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtkA+QtTREyIX+iI1vpAdN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fawzy-AI-Explorer/X-From-Scratch/blob/main/Linear_Regression-From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression From Scratch"
      ],
      "metadata": {
        "id": "0kvJneJdrMQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goals\n",
        "In this toturial, we will:\n",
        "\n",
        "- Implement the linear regression model\n",
        "- Implement the gradient descent algorithm to train the model\n",
        "- Implement $R^2 Score$ to calculate the accuracy<br><br>\n",
        "\n",
        "*All from scratch*"
      ],
      "metadata": {
        "id": "saplMwKGrVBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools\n",
        "In this toturial, we will make use of:\n",
        "- math, This module provides access to the mathematical functions defined by the C standard\n",
        "- pandas, a Python library used for working with data sets\n",
        "- NumPy, a popular library for scientific computing\n",
        "- seaborn, a Python data visualization library based on matplotlib\n",
        "- Matplotlib, a popular library for plotting data"
      ],
      "metadata": {
        "id": "d2VFcTdTseFO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "Suy_k-6rq8gl"
      },
      "outputs": [],
      "source": [
        "import math, copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Define a linear regressino class.\"\"\"\n",
        "\n",
        "\n",
        "class LinearRegression():\n",
        "    \"\"\"Representation of a linear regression model.\n",
        "    the model predicts a number form  infinitely many possible outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=1e-4, max_iter=1000):\n",
        "      \"\"\"Initialize the linear regression model\n",
        "\n",
        "        Args:\n",
        "          learning_rage (float) : The number indicates the step in gradient descent.\n",
        "          max_iter (int) : The maximum number of passes over the training data.\n",
        "      \"\"\"\n",
        "\n",
        "      self.learning_rate = learning_rate\n",
        "      self.max_iter = max_iter\n",
        "      self.w, self.b = None, 0.\n",
        "\n",
        "    def fit(self, X, y):\n",
        "      \"\"\"\n",
        "      Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
        "      num_iters gradient steps with learning rate alpha\n",
        "\n",
        "      Args:\n",
        "        X (ndarray (m,n))   : Data, m examples with n features\n",
        "        y (ndarray (m,))    : target values\n",
        "\n",
        "      Returns:\n",
        "        self (object) : Fitted model estimator.\n",
        "        \"\"\"\n",
        "\n",
        "      # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "      J_history = []\n",
        "      W_history = []\n",
        "      b_history = []\n",
        "      n = 1\n",
        "      if (len(X.shape) > 1):\n",
        "        n = X.shape[1]\n",
        "      self.w = np.zeros((n, ))\n",
        "      self.b = 0.\n",
        "\n",
        "      for i in range(self.max_iter):\n",
        "        dj_dw, dj_db = self.compute_gradient(X, y)\n",
        "        self.w = self.w - (self.learning_rate * dj_dw)\n",
        "        self.b -= self.learning_rate * dj_db\n",
        "\n",
        "        if i < 100000:\n",
        "          J_history.append(self.compute_cost(X, y))\n",
        "          W_history.append(copy.deepcopy(self.w))\n",
        "          b_history.append(copy.deepcopy(self.b))\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i % math.ceil(self.max_iter / 10) == 0:\n",
        "          print(\"{:>8}  cost={:>1.5e}\".format(i, self.compute_cost(X, y)), end='')\n",
        "          for i in range(n):\n",
        "            print(\"  w[{}]={:>1.1e}\".format(i, self.w[i]), end='')\n",
        "          print(\"  b={:>1.1e}\".format(self.b), end='')\n",
        "          for i in range(n):\n",
        "            print(\"  dj_dw[{}]={:>1.1e}\".format(i, dj_dw[i]), end='')\n",
        "          print(\"  dj_db={:>1.1e}\".format(dj_db))\n",
        "\n",
        "      return self\n",
        "\n",
        "\n",
        "    def compute_gradient(self, X, y):\n",
        "      \"\"\"\n",
        "      Computes the gradient for linear regression\n",
        "      Args:\n",
        "        X (ndarray (m,n)): Data, m examples with n features\n",
        "        y (ndarray (m,)) : target values\n",
        "\n",
        "      Returns:\n",
        "        dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "        dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "      \"\"\"\n",
        "\n",
        "      m = X.shape[0]\n",
        "      n = 1\n",
        "      if (len(X.shape) > 1):\n",
        "        n = X.shape[1]\n",
        "      dj_dw = np.zeros((n,))\n",
        "      dj_db = 0.\n",
        "\n",
        "      for i in range(m):\n",
        "        f_wb_i = self.predict(X[i])\n",
        "        err = (f_wb_i - y[i])\n",
        "        dj_dw = dj_dw + (err * X[i])\n",
        "        dj_db += err\n",
        "\n",
        "      dj_dw /= m\n",
        "      dj_db /= m\n",
        "\n",
        "      return dj_dw, dj_db\n",
        "\n",
        "\n",
        "    def compute_cost(self, X, y):\n",
        "      \"\"\"\n",
        "      Computes the cost function for linear regression.\n",
        "\n",
        "      Args:\n",
        "        X (ndarray (m,n)): Data, m examples and n features\n",
        "        y (ndarray (m,)): target values\n",
        "\n",
        "      Returns\n",
        "          total_cost (float): The cost of using w,b as the parameters for linear regression\n",
        "                to fit the data points in x and y\n",
        "      \"\"\"\n",
        "\n",
        "      m = X.shape[0]\n",
        "      cost = 0.0\n",
        "      for i in range(m):\n",
        "        f_wb_i = self.predict(X[i])\n",
        "        cost += (f_wb_i - y[i]) ** 2\n",
        "\n",
        "      cost /= (2 * m)\n",
        "\n",
        "      return cost\n",
        "\n",
        "    def predict(self, X):\n",
        "      \"\"\"\n",
        "      single predict using linear regression\n",
        "      Args:\n",
        "        X (ndarray): Shape (m, n) example with multiple features\n",
        "\n",
        "      Returns:\n",
        "        p (scalar):  prediction\n",
        "      \"\"\"\n",
        "\n",
        "      p = np.dot(X, self.w) + self.b\n",
        "      return p\n",
        "\n",
        "    def score(self, X, y):\n",
        "      \"\"\"\n",
        "      Returns the accuracy of the model\n",
        "      Args:\n",
        "      X (ndarray): Shape(m, n) examples with multiple features\n",
        "      y (ndarray): Shape (m,) the actual target values\n",
        "\n",
        "      Returns:\n",
        "        accuracy (scalar): the accuracy of the model\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = 1\n",
        "      if (len(X.shape) > 1):\n",
        "        n = X.shape[1]\n",
        "      RSS = 0.\n",
        "      TSS = 0.\n",
        "      y_mu = np.mean(y)\n",
        "\n",
        "      for i in range(m):\n",
        "          RSS += (y[i] - self.predict(X[i])) ** 2\n",
        "          TSS += (y[i] - y_mu) ** 2\n",
        "\n",
        "      score = 1 - RSS / TSS\n",
        "      return round(score, 2)\n",
        "\n",
        "    def get_params(self):\n",
        "      \"\"\"\n",
        "      Get parameters for this estimator.\n",
        "\n",
        "      Returns:\n",
        "        params (dict) : Parameter names mapped to their values.\n",
        "      \"\"\"\n",
        "      params = {}\n",
        "      params[\"learning_rate\"] = self.learning_rate\n",
        "      params[\"max_iter\"] = self.max_iter\n",
        "      if self.w is not None:\n",
        "        for i in range(len(self.w)):\n",
        "            p_name = \"w\" + str(i)\n",
        "            params[p_name] = self.w[i]\n",
        "      if self.b is not None:\n",
        "        params[\"b\"] = self.b\n",
        "\n",
        "      return params\n",
        "\n",
        "    def set_params(self, **params):\n",
        "      \"\"\"\n",
        "      Set the parameters of this estimator.\n",
        "\n",
        "      Args:\n",
        "        Estimator parameters.\n",
        "\n",
        "      Returns:\n",
        "        self (estimator instance) : Estimator instance.\n",
        "      \"\"\"\n"
      ],
      "metadata": {
        "id": "57Ab-TcG4Xdo"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test the model"
      ],
      "metadata": {
        "id": "UtIFpJl6LiX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data set\n",
        "url = 'https://raw.githubusercontent.com/Ad7amstein/Linear_Regression-E-commerce/main/Ecommerce%20Customers.csv'\n",
        "data = pd.read_csv(url)\n",
        "# extract the four main features\n",
        "X = data.values[:, 3:7]\n",
        "y = data.values[:, 7]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "lr2 = LinearRegression(max_iter=10000, learning_rate=1e-4)\n",
        "lr2.fit(X_train, y_train)\n",
        "print(\"The score of our model is {}%\".format(lr2.score(X_test, y_test) * 100))"
      ],
      "metadata": {
        "id": "imhqR7VXo4-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ec4a19-660c-4943-993b-ad2e4074752e"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       0  cost=7.18746e+04  w[0]=1.7e+00  w[1]=6.1e-01  w[2]=1.9e+00  w[3]=1.9e-01  b=5.0e-02  dj_dw[0]=-1.7e+04  dj_dw[1]=-6.1e+03  dj_dw[2]=-1.9e+04  dj_dw[3]=-1.9e+03  dj_db=-5.0e+02\n",
            "    1000  cost=2.49010e+03  w[0]=6.9e+00  w[1]=5.5e+00  w[2]=4.9e+00  w[3]=6.9e+00  b=1.5e-01  dj_dw[0]=-5.7e+00  dj_dw[1]=-3.0e+01  dj_dw[2]=2.1e+01  dj_dw[3]=-5.9e+01  dj_db=4.4e-01\n",
            "    2000  cost=2.05676e+03  w[0]=7.5e+00  w[1]=8.4e+00  w[2]=3.0e+00  w[3]=1.2e+01  b=1.0e-01  dj_dw[0]=-5.1e+00  dj_dw[1]=-2.7e+01  dj_dw[2]=1.9e+01  dj_dw[3]=-5.3e+01  dj_db=4.4e-01\n",
            "    3000  cost=1.70796e+03  w[0]=8.0e+00  w[1]=1.1e+01  w[2]=1.2e+00  w[3]=1.7e+01  b=5.9e-02  dj_dw[0]=-4.5e+00  dj_dw[1]=-2.5e+01  dj_dw[2]=1.7e+01  dj_dw[3]=-4.7e+01  dj_db=4.4e-01\n",
            "    4000  cost=1.42722e+03  w[0]=8.4e+00  w[1]=1.3e+01  w[2]=-3.7e-01  w[3]=2.2e+01  b=1.5e-02  dj_dw[0]=-4.0e+00  dj_dw[1]=-2.2e+01  dj_dw[2]=1.5e+01  dj_dw[3]=-4.2e+01  dj_db=4.4e-01\n",
            "    5000  cost=1.20125e+03  w[0]=8.8e+00  w[1]=1.5e+01  w[2]=-1.8e+00  w[3]=2.6e+01  b=-2.9e-02  dj_dw[0]=-3.6e+00  dj_dw[1]=-2.0e+01  dj_dw[2]=1.3e+01  dj_dw[3]=-3.8e+01  dj_db=4.4e-01\n",
            "    6000  cost=1.01935e+03  w[0]=9.1e+00  w[1]=1.7e+01  w[2]=-3.0e+00  w[3]=2.9e+01  b=-7.2e-02  dj_dw[0]=-3.2e+00  dj_dw[1]=-1.8e+01  dj_dw[2]=1.2e+01  dj_dw[3]=-3.4e+01  dj_db=4.3e-01\n",
            "    7000  cost=8.72939e+02  w[0]=9.4e+00  w[1]=1.9e+01  w[2]=-4.2e+00  w[3]=3.3e+01  b=-1.2e-01  dj_dw[0]=-2.8e+00  dj_dw[1]=-1.6e+01  dj_dw[2]=1.1e+01  dj_dw[3]=-3.0e+01  dj_db=4.3e-01\n",
            "    8000  cost=7.55080e+02  w[0]=9.7e+00  w[1]=2.1e+01  w[2]=-5.2e+00  w[3]=3.6e+01  b=-1.6e-01  dj_dw[0]=-2.5e+00  dj_dw[1]=-1.5e+01  dj_dw[2]=9.6e+00  dj_dw[3]=-2.7e+01  dj_db=4.3e-01\n",
            "    9000  cost=6.60205e+02  w[0]=9.9e+00  w[1]=2.2e+01  w[2]=-6.1e+00  w[3]=3.8e+01  b=-2.0e-01  dj_dw[0]=-2.2e+00  dj_dw[1]=-1.3e+01  dj_dw[2]=8.6e+00  dj_dw[3]=-2.4e+01  dj_db=4.3e-01\n",
            "The score of our model is 82.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FjHHRslLMCF8"
      },
      "execution_count": 128,
      "outputs": []
    }
  ]
}